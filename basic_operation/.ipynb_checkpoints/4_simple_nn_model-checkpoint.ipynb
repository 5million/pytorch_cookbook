{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6f0d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cadc102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bbbe4ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f0de9",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "When presented with some training data, our untrained network is likely not to give the correct answer. Loss function measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
    "\n",
    "Common loss functions include nn.MSELoss (Mean Square Error) for regression tasks, and nn.NLLLoss (Negative Log Likelihood) for classification. nn.CrossEntropyLoss combines nn.LogSoftmax and nn.NLLLoss.\n",
    "\n",
    "We pass our modelâ€™s output logits to nn.CrossEntropyLoss, which will normalize the logits and compute the prediction error.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3d12357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9bfdc9",
   "metadata": {},
   "source": [
    "## optimizer\n",
    "Inside the training loop, optimization happens in three steps:\n",
    "* Call optimizer.zero_grad() to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
    "* Backpropagate the prediction loss with a call to loss.backward(). PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
    "* Once we have our gradients, we call optimizer.step() to adjust the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de41a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5a146d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(data_loader, model, loss_fn, optimizer):\n",
    "    size = len(data_loader.dataset)\n",
    "    for batch, (X, y) in enumerate(data_loader):\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "            \n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "843654f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e30bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.294921  [    0/60000]\n",
      "loss: 2.293026  [ 6400/60000]\n",
      "loss: 2.277829  [12800/60000]\n",
      "loss: 2.277014  [19200/60000]\n",
      "loss: 2.250424  [25600/60000]\n",
      "loss: 2.223075  [32000/60000]\n",
      "loss: 2.231553  [38400/60000]\n",
      "loss: 2.191958  [44800/60000]\n",
      "loss: 2.185193  [51200/60000]\n",
      "loss: 2.175397  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 40.2%, Avg loss: 2.156733 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.153295  [    0/60000]\n",
      "loss: 2.156245  [ 6400/60000]\n",
      "loss: 2.102339  [12800/60000]\n",
      "loss: 2.126546  [19200/60000]\n",
      "loss: 2.075315  [25600/60000]\n",
      "loss: 2.013240  [32000/60000]\n",
      "loss: 2.043741  [38400/60000]\n",
      "loss: 1.964818  [44800/60000]\n",
      "loss: 1.957394  [51200/60000]\n",
      "loss: 1.911524  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 1.898060 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.914265  [    0/60000]\n",
      "loss: 1.900998  [ 6400/60000]\n",
      "loss: 1.785880  [12800/60000]\n",
      "loss: 1.835323  [19200/60000]\n",
      "loss: 1.726408  [25600/60000]\n",
      "loss: 1.671811  [32000/60000]\n",
      "loss: 1.696909  [38400/60000]\n",
      "loss: 1.592655  [44800/60000]\n",
      "loss: 1.609212  [51200/60000]\n",
      "loss: 1.520530  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Avg loss: 1.525669 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.579341  [    0/60000]\n",
      "loss: 1.556809  [ 6400/60000]\n",
      "loss: 1.403152  [12800/60000]\n",
      "loss: 1.486804  [19200/60000]\n",
      "loss: 1.365019  [25600/60000]\n",
      "loss: 1.357591  [32000/60000]\n",
      "loss: 1.372502  [38400/60000]\n",
      "loss: 1.292257  [44800/60000]\n",
      "loss: 1.328012  [51200/60000]\n",
      "loss: 1.235178  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Avg loss: 1.254007 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.324464  [    0/60000]\n",
      "loss: 1.316535  [ 6400/60000]\n",
      "loss: 1.146724  [12800/60000]\n",
      "loss: 1.261734  [19200/60000]\n",
      "loss: 1.137769  [25600/60000]\n",
      "loss: 1.160198  [32000/60000]\n",
      "loss: 1.177486  [38400/60000]\n",
      "loss: 1.111253  [44800/60000]\n",
      "loss: 1.152168  [51200/60000]\n",
      "loss: 1.072753  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 1.088443 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726c6a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
